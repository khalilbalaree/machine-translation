# -*- coding: utf-8 -*-
"""LSTMN2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o4e3BYUE_UA0a491NFNcbr-6sQ_eNbVt
"""

import torch
from torch import nn
import torch.utils.data

torch.manual_seed(11)
device = torch.device("cuda")

from google.colab import drive
drive.mount('/content/drive')
class LSTMN(nn.Module):

    ### PARAMETERS
    # dim is number of hidden units
    # max_len is the maximum length of the input
    # batch_size is length of minibatch
    # output_size is dim of final output
    # init_hidden (optional) is the initial hidden state
    # init_cell (optional) is the initial cell state
    ### RETURNS
    # outputs, size (batch_size, output_size)
    # hidden_states, size (batch_size, max_length, dim)
    ###

    def __init__(self, dim, max_len, batch_size, output_size, hidden_size, init_hidden=0, init_cell=0 ):
        super(LSTMN, self).__init__()
        self.embedding = nn.Embedding(hidden_size,
                                      hidden_size).to(device)  #TODO 暂时这么写
        self.max_len = max_len
        self.dim = dim
        self.output_size = output_size
        self.batch_size = batch_size
        '''
        embedding from encoder
        '''
        self.hidden_size = hidden_size

        # Weight matrix for each of the 4 gates
        self.W = torch.randn(5 * dim, dim + dim, device=device,
                             dtype=torch.float)  # modifed....  TODO should be max_len + dim

        # Bias matrix
        self.b = torch.randn(5 * dim, device=device, dtype=torch.float)

        # Set forget bias to 1 per http://proceedings.mlr.press/v37/jozefowicz15.pdf
        self.b[:dim] = 1

        # initial hidden state
        self.h = torch.randn(dim, device=device, dtype=torch.float) if init_hidden == 0 else init_hidden

        # initial cell state
        self.c = torch.randn(dim, device=device, dtype=torch.float) if init_cell == 0 else init_cell

        # Attention weights
        self.W_h = torch.zeros(10, dim, device=device, dtype=torch.float)  # todo init weight in zeros
        self.W_h2 = torch.zeros(10, dim, device=device, dtype=torch.float)  # todo init weight in zeros
        self.W_x = torch.randn(10, dim, device=device, dtype=torch.float)  # todo should be max_len
        self.W_ht = torch.randn(10, dim, device=device, dtype=torch.float)

        self.v = torch.randn(10, device=device, dtype=torch.float)

        self.ht = torch.randn(dim, device=device, dtype=torch.float)
        self.ct = torch.randn(dim, device=device, dtype=torch.float)

        # Projection layer
        self.repetiveProjection = torch.nn.Linear(dim * 5, dim * 5).to(device)
        self.reshape_bef = torch.nn.Linear(dim * 5, 1600).to(device)
        self.reshape_pos = torch.nn.Linear(1600, dim * 5).to(device)
        self.reshape_bef2 = torch.nn.Linear(dim, 400).to(device)
        self.reshape_pos2 = torch.nn.Linear(400, dim).to(device)
        self.repetiveProjection3 = torch.nn.Linear(dim * 4, dim * 4).to(device)
        self.inter_projection_groups = nn.ModuleList()
        for i in range(max_len):
            self.inter_projection_groups.append(
                nn.Sequential(
                    torch.nn.Linear(dim * 5, dim * 10).to(device),
                    nn.ReLU().to(device),
                    nn.Dropout(0.1).to(device),
                    nn.BatchNorm2d(1).to(device),
                    torch.nn.Linear(dim * 10, dim * 5).to(device),
                    nn.ReLU().to(device),
                    nn.Dropout(0.1).to(device),
                    nn.BatchNorm2d(1).to(device)
                )
            )
        self.inter_projection_groups1_rep = nn.Sequential(
                    torch.nn.Conv2d(1, 3, 3, 1, 1).to(device),
                    nn.ReLU().to(device),
                    nn.Dropout(0.1).to(device),
                    nn.BatchNorm2d(3).to(device),
                    torch.nn.Conv2d(3, 32, 3, 1, 1).to(device),
                    nn.ReLU().to(device),
                    nn.Dropout(0.1).to(device),
                    nn.BatchNorm2d(32).to(device),
                    torch.nn.Conv2d(32, 3, 3, 1, 1).to(device),
                    nn.ReLU().to(device),
                    nn.Dropout(0.1).to(device),
                    nn.BatchNorm2d(3).to(device),
                    torch.nn.Conv2d(3, 1, 3, 1, 1).to(device),
                    nn.ReLU().to(device),
                    nn.Dropout(0.1).to(device),
                    nn.BatchNorm2d(1).to(device),
            )
        self.inter_projection_groups2_rep = nn.Sequential(
            torch.nn.Conv2d(1, 32, 3, 1, 1).to(device),
            nn.ReLU().to(device),
            nn.Dropout(0.1).to(device),
            nn.BatchNorm2d(32).to(device),
            torch.nn.Conv2d(32, 64, 3, 1, 1).to(device),
            nn.ReLU().to(device),
            nn.Dropout(0.1).to(device),
            nn.BatchNorm2d(64).to(device),
            torch.nn.Conv2d(64, 64, 3, 1, 1).to(device),
            nn.ReLU().to(device),
            nn.Dropout(0.1).to(device),
            nn.BatchNorm2d(64).to(device),
            torch.nn.Conv2d(64, 64, 3, 1, 1).to(device),
            nn.ReLU().to(device),
            nn.Dropout(0.1).to(device),
            nn.BatchNorm2d(64).to(device),
            torch.nn.Conv2d(64, 64, 3, 1, 1).to(device),
            nn.ReLU().to(device),
            nn.Dropout(0.1).to(device),
            nn.BatchNorm2d(64).to(device),
            torch.nn.Conv2d(64, 32, 3, 1, 1).to(device),
            nn.ReLU().to(device),
            nn.Dropout(0.1).to(device),
            nn.BatchNorm2d(32).to(device),
            torch.nn.Conv2d(32, 1, 3, 1, 1).to(device),
            nn.ReLU().to(device),
            nn.Dropout(0.1).to(device),
            nn.BatchNorm2d(1).to(device),
        )
        self.pos_cat = nn.Sequential(
                torch.nn.Conv2d(2, 1, 3, 1, 1).to(device),
                nn.BatchNorm2d(1).to(device)
        )

    # x is batch of input column vectors shape (batch_size, max_len)
    def forward(self, x, hidden_states, past_ht, past_x, past_c, counter, encoder_ct=0):
        '''
        embedding batch input
        '''
        if type(encoder_ct) != int and counter < 1:
            x = self.embedding(x).view(1, 1, -1)
        '''embedded word'''
        # print("\n\nnew iteration of each row")
        # print('x_index: ', x_index)  TODO each embedded words
        # print('x_t: ', x_t, 'x_t shape: ', x_t.shape)
        '''pre conv block'''
        '''
        x_t = self.reshape_bef2(x)
        x_t = x_t.view(1, 1, 20, 20)
        x_t = self.inter_projection_groups1_rep(x_t)
        x_t = x_t.view(-1)
        x_t = self.reshape_pos2(x_t)
        x_t = x_t.view(1,1,300)
        '''

        x_t = x.to('cuda').squeeze(0).squeeze(0)  # 【300】
        attention_vector = []

        # Iterate through past hidden states and calculate attention vector
        for k, h in enumerate(hidden_states):
            a_t = self.v @ (nn.Tanh()(self.W_h @ h + self.W_x @ x_t + self.W_h2 @ past_x[k] + self.W_ht @ past_ht[
                k]))  # todo add w_x @ x_t term
            attention_vector.append(a_t)

        attention_vector = torch.Tensor(attention_vector)
        attention_softmax = torch.nn.Softmax()(attention_vector)  # 同 torch.nn.functional.softmax(attention_vector)
        # print('hidden state: ', hidden_states) #accumulative ==> iteration x 64
        # print('attention vector; ', attention_vector)#accumulative
        # print('atten vector softmax: ',attention_softmax)#accumulative
        if len(hidden_states) > 0:
            ht = 0
            ct = 0
            for k, s in enumerate(attention_softmax):
                ht += s * hidden_states[k]
                ct += s * past_c[k]

            self.ht = torch.tensor(ht, device='cuda')
            self.ct = torch.tensor(ct, device='cuda')

        # print('self.ht: ',self.ht.shape )  # 64 x 1
        # print('self.ct:', self.ct) # 64 x 1

        concat_input = torch.cat((self.ht, x_t.to('cuda')), 0).view(-1, 1)  # 74 x 1

        # print(concat_input.shape)    #TODO 512 , 1
        # print(self.ht.shape)
        # print(x_t.to('cuda').shape)
        # print(self.W.shape)

        # concat_input = self.inter_projection_groups2[counter](concat_input.view(-1))
        whx = self.W.mm(concat_input).view(-1) + self.b
        '''inter conv block'''
        
        whx = self.reshape_bef(whx)
        whx_pre = whx.view(1, 1, 40, 40)
        whx = self.inter_projection_groups2_rep(whx_pre)  # TODO linear here ， 目前最好效果！！！！！！
        whx = torch.cat([whx_pre,whx],1)  #residual block
        whx = self.pos_cat(whx)
        whx = whx.view(-1)
        whx = self.reshape_pos(whx)
        whx = whx.view(1500)
        

        f_t = nn.Sigmoid()(whx[:self.dim])
        o_t = nn.Sigmoid()(whx[self.dim:self.dim * 2])
        i_t = nn.Sigmoid()(whx[self.dim * 2:self.dim * 3])
        ch_t = nn.Tanh()(whx[self.dim * 3:self.dim * 4])
        r_t = nn.Sigmoid()(whx[self.dim * 4:])
        self.c = f_t * self.ct + i_t * ch_t  # + r_t * self.ht

        if type(encoder_ct) != int and len(encoder_ct) > counter:  # todo easy deep fusion
            # print(torch.sum(r_t * self.ct))
            # print(torch.sum(f_t * self.ct))
            # print(torch.sum(i_t * ch_t))
            # print(torch.sum(encoder_ct[counter]))
            # print("########")
            self.c += encoder_ct[counter]
        self.h = o_t * (nn.Tanh()(self.c))

        past_x.append(x_t)
        past_ht.append(self.ht)
        hidden_states.append(self.h)
        past_c.append(self.c)

        # print('self.h: ',self.h.shape)    # 64 x 1
        # print('self.c: ', self.c.shape)   #64 x 1
        # print('past_ht: ', past_ht)
        return hidden_states[-1], hidden_states, past_ht, past_x, past_c, counter


"""
Created on Fri Feb 15 06:20:25 2019

@author: Administrator
"""

from io import open
import unicodedata
import string
import re
import random
import argparse
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

"""
Created on Fri Feb 15 06:20:25 2019

@author: Administrator
"""

from io import open
import unicodedata
import string
import re
import random
import argparse
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")
print("It's running by:", device)
# device = torch.device("cpu")


"""parser = argparse.ArgumentParser(description='This is the description')

#learning
learn = parser.add_argument_group('learning options')
learn.add_argument('--lr', type= float, default= 0.001, help = 'learning rate')
learn.add_argument('--dropout', type = float, default = 0.1, help = 'Dropout rate')
learn.add_argument('--hidden_size', type = int, default = 256, help = 'size of hidden_layer')

#language processing
lang = parser.add_argument_group('language processing options')
lang.add_argument('--MAX_LENGTH', type = int, default = 10, help = ' Here the maximum length is 10 words to simplify training.')
lang.add_argument('--data_path', type = str, default = '%s-%s.txt', help = 'Path of training data' )
lang.add_argument('--input_lang',type = str, default = 'eng', help = 'input language name')
lang.add_argument('--output-lang', type = str, default = 'spa', help = 'output language name')

#Training arguments
trainArgs = parser.add_argument_group('Training argument')
trainArgs.add_argument('--n_iters', type = int, default =100000, help = 'number of iters times') #75000
trainArgs.add_argument('--print_every', type = int, default = 1000, help = 'to print results after how many times run ') # 5000

args = parser.parse_args()
"""


class Args:
    lr = 0.001
    dropout = 0.1
    hidden_size = 300
    MAX_LENGTH = 30
    data_path = '%s-%s.txt'
    input_lang = 'eng'
    output_lang = 'spa'
    n_iters = 150000
    print_every = 1000


args = Args()

SOS_token = 0
EOS_token = 1

MAX_LENGTH = 30

# ------------------------------------------------------------------------------
class EncoderRNN(nn.Module):
    def __init__(self, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, embedded, hidden):
        output = embedded.view(1, 1, -1)

        output, hidden = self.gru(output, hidden)

        return output, hidden, embedded

    def initHidden(self):  # initial Hidden-layer
        return torch.zeros(1, 1, self.hidden_size, device=device)


class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output = self.gru(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden


class AttnDecoderRNN(nn.Module):  # 'Neural Machine Translation by Jointly Learning to Align and Translate'
    def __init__(self, hidden_size, output_size, dropout_p=args.dropout, max_length=args.MAX_LENGTH):  #
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.output_size,
                                      self.hidden_size)  # A simple lookup table that stores embeddings of a fixed dictionary and size.
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)  #
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

        self.tanh = nn.Tanh()

    def forward(self, embedded, hidden, encoder_outputs):
        # embedded = self.embedding(embedded).view(1, 1, -1)
        # embedded = self.dropout(embedded)
        '''
        Intra attention goes here read from decoder input
        '''

        # TODO inter attention from here
        # print('embedded[0]',embedded[0].shape)  # 1, 256
        # print( ' hidden[0]', hidden[0].shape)  # 1, 256
        attn_weights = F.softmax(
            F.tanh(
                self.attn(torch.cat((embedded[0], hidden[0]), 1))
            )
            , dim=1)
        # print(attn_weights.shape)
        # print(encoder_outputs.shape)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)
        # print('output', output.shape)  # 1, 1, 256
        # TODO till here

        output = F.relu(output)
        output, hidden = self.gru(output, hidden)

        output = self.out(output[0])  # FC
        return output, hidden, attn_weights

    def initHidden(self):  # Initial hidden-layer
        return torch.zeros(1, 1, self.hidden_size, device=device)


teacher_forcing_ratio = 0.5


def train(input_tensor, target_tensor, encoder, decoder, lstmn, lstmn2, criterion, max_length=args.MAX_LENGTH):
    encoder_hidden = encoder.initHidden()  # init hidden layer

    input_tensor = torch.tensor(input_tensor, device=device, dtype=torch.float)
    target_tensor = torch.tensor(target_tensor, device=device, dtype=torch.float)
    # print(input_tensor.shape)
    # print(target_tensor.shape)
    input_length = input_tensor.shape[0]
    target_length = target_tensor.shape[0]

    # encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0
    # print('input tensor shape: ', input_tensor.shape)
    # print('input tensor: ', input_tensor)

    '''
    for ei in range(input_length): # word by word
        encoder_output, encoder_hidden, encoder_embedded = encoder(input_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]  #store the outputs in sequence
        encoder_embeddeds[ei] = encoder_embedded[0,0]
        #print('encoder output: ', encoder_output[0,0])
        #print('encoder hidden: ', encoder_hidden) only need the last hidden and entire output

    '''

    decoder_hidden = encoder_hidden

    '''
    #test lstmn
    #test lstmn
    #test lstmn
    '''
    # print('\n\nhere is lstmn\n\n')
    decoder_input = torch.tensor([[SOS_token]], device=device)

    lstmn_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)  # TODO 原来是 max length
    # print('encodered_ embeddeds: ', encoder_embeddeds)
    # print('encodered_ embeddeds shape: ', encoder_embeddeds.shape)
    hidden_states = []
    past_ht = []
    past_x = []
    past_c = []
    counter = 0
    for ei in range(input_length):
        outputs, hidden_states, past_ht, past_x, past_c, counter = lstmn(input_tensor[ei], hidden_states, past_ht,
                                                                         past_x, past_c, counter, 0)
        lstmn_outputs[ei] = outputs
        counter += 1
    lstmn_hidden = outputs.unsqueeze(0).unsqueeze(0)  # TODO 一种处理方式，似乎好一点 改用此方法前10%下降至 7.86 -> 7.8
    # print('output hidden: ', hidden_states[0])

    # print('lstmn_outputs: ', lstmn_outputs)
    # print('lstmn_hidden: ',lstmn_hidden.shape)
    # print(1+'1')
    # TODO 加了 很多 linear 后 效果提升至 7.89 - > 6.68
    decoder_hidden = lstmn_hidden
    encoder_outputs = lstmn_outputs

    # use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    #    if use_teacher_forcing:
    #        # Teacher forcing: Feed the target as the next input
    #        for di in range(target_length):
    #            decoder_output, decoder_hidden, decoder_attention = decoder(
    #                decoder_input, decoder_hidden, encoder_outputs)
    #            loss += criterion(decoder_output, target_tensor[di])
    #            decoder_input = target_tensor[di]  # Teacher forcing
    #
    #    else:
    # Without teacher forcing: use its own predictions as the next input

    hidden_states = []
    past_ht = []
    past_x = []
    past_c2 = []
    counter = 0
    for di in range(target_length):
        outputs, hidden_states, past_ht, past_x, past_c2, counter = lstmn2(decoder_input, hidden_states, past_ht,
                                                                           past_x, past_c2, counter, past_c)
        counter += 1
        decoder_output, decoder_hidden, decoder_attention = decoder(outputs.unsqueeze(0).unsqueeze(0), decoder_hidden,
                                                                    encoder_outputs)
        # print(decoder_output.shape)

        # print(decoder_output.shape)
        # print(decoder_input.shape)
        # print(target_tensor[di].shape)
        y = torch.ones(1).to(device)
        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0), y)

        # print(random.random())
        # if random.random() < teacher_forcing_ratio:  #teaching force applied
        decoder_input = decoder_output.squeeze()
        # else:
        # decoder_input = target_tensor[di]
        # print(1 + '1')

    # print(loss)
    loss.backward()

    return loss.item() / target_length


# --------------------print time consume-------------------------------------------------------------
import time
import math


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))


# --------------------------------------------------------------------------------------------

def trainIters(encoder, decoder, lstmn, lstmn2, n_iters, print_every=100, plot_every=10, learning_rate=args.lr):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)
    # TODO 目前最好效果 SGD = 0.01， Adam = 0.001
    lstmn_optimizer = optim.SGD(lstmn.parameters(),lr=0.01)  # list(lstmn.parameters()) + list(decoder.parameters())
    lstmn_optimizer2 = optim.SGD(lstmn2.parameters(), lr=0.01)


    training_pairs = []
    eng = np.load('/content/drive/My Drive/lstmn/emb_en.npy',allow_pickle=True)
    chn = np.load('/content/drive/My Drive/lstmn/emb_zh.npy',allow_pickle=True)
    counter = 0
    while counter < n_iters:
        randomidx = random.randint(1, 14999)
        len_eng = eng[randomidx].shape
        len_chn = chn[randomidx].shape
        if len_eng[0] != 0 and len_chn[0] != 0:
            # print(eng[randomidx].shape, chn[randomidx].shape, len_eng[0], len_chn[0])
            emb_pair = (eng[randomidx], chn[randomidx])
            counter += 1
        training_pairs.append(emb_pair)

    criterion = torch.nn.CosineEmbeddingLoss(reduction='none')  # nn.MSELoss()#nn.NLLLoss()
    for iter in range(1, n_iters + 1):

        if 60000 > iter > 30000:
            for param_group in lstmn_optimizer.param_groups:
                param_group['lr'] = 0.001
        elif iter >= 60000:
            for param_group in lstmn_optimizer.param_groups:
                param_group['lr'] = 0.0001

        training_pair = training_pairs[iter - 1]
        # print(training_pair)
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]
        # encoder.train()
        decoder.train()
        lstmn.train()
        lstmn2.train()
        lstmn_optimizer.zero_grad()
        lstmn_optimizer2.zero_grad()
        # encoder_optimizer.zero_grad()
        decoder_optimizer.zero_grad()

        loss = train(input_tensor, target_tensor, encoder,
                     decoder, lstmn, lstmn2, criterion)

        lstmn_optimizer.step()
        lstmn_optimizer2.step()
        # encoder_optimizer.step()
        decoder_optimizer.step()

        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0 :  # print results
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))

        if iter % plot_every == 0:  # plot results
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0

    showPlot(plot_losses)
    # torch.save(encoder.state_dict(), './model_parameters/encoder.weights') #Returns a dictionary containing a whole state of the module.
    # torch.save(decoder.state_dict(), './model_parameters/decoder.weights')


# ------------------------------------------------------------------------------
# plot the result

import matplotlib.pyplot as plt

plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np


def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)


# ------------------------------------------------------------------------------
# evaluate the result
def evaluate(encoder, decoder, sentence, max_length=args.MAX_LENGTH):
    with torch.no_grad():
        input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei],
                                                     encoder_hidden)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden

        decoded_words = []
        decoder_attentions = torch.zeros(max_length, max_length)

        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            decoder_attentions[di] = decoder_attention.data
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words, decoder_attentions[:di + 1]




# ------------------------------------------------------------------------------

# hidden_size = 256
max_len = 30
dim = args.hidden_size
batch_size = 1
encoder1 = EncoderRNN(args.hidden_size).to(device)  # Enconder
attn_decoder1 = AttnDecoderRNN(args.hidden_size, dim, dropout_p=args.dropout).to(
    device)  # original is output_lang.n_words
# attn_decoder1 = DecoderRNN(args.hidden_size, output_lang.n_words).to(device)


# output_size = args.hidden_size
lstmn = LSTMN(dim, max_len, batch_size, dim, dim)
lstmn2 = LSTMN(dim, max_len, batch_size, dim, dim)

# ------------Load trained weights-------------------------------
# encoder1.load_state_dict(torch.load('./encoder.weights'))
# attn_decoder1.load_state_dict(torch.load('./decoder.weights'))

encoder1 = encoder1.eval()
attn_decoder1 = attn_decoder1.eval()
# ------------------------------------------------------

# if torch.cuda.device_count() >1: # Multi-GPUs
#    print("Let's use", torch.cuda.device_count(),"GPUs!")
#    encoder1 = nn.DataParallel(encoder1)
#    attn_decoder1 = nn.DataParallel(attn_decoder1)
# encoder1.to(device)
# attn_decoder1.to(device)
torch.manual_seed(1)
torch.cuda.manual_seed_all(1)
random.seed(1)
np.random.seed(1)
torch.backends.cudnn.deterministic = True

trainIters(encoder1, attn_decoder1, lstmn, lstmn2, args.n_iters, print_every=args.print_every)  # hyperparameters